{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d365ab",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "This notebook builds the merged news–price dataset and cleans the FNSPID headlines for downstream modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5c94a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1 of 86 missing tickers: ['DEJ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Financial News Sentiment Analysis\\.venv\\Lib\\site-packages\\yfinance\\scrapers\\history.py:201: Pandas4Warning: Timestamp.utcnow is deprecated and will be removed in a future version. Use Timestamp.now('UTC') instead.\n",
      "  dt_now = pd.Timestamp.utcnow()\n",
      "$DEJ: possibly delisted; no price data found  (1d 2011-04-28 -> 2020-06-11)\n",
      "\n",
      "1 Failed download:\n",
      "['DEJ']: possibly delisted; no price data found  (1d 2011-04-28 -> 2020-06-11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No price data returned for DEJ; skipping\n",
      "Merged dataset written to D:\\Financial News Sentiment Analysis\\Data\\merged_news_prices.csv with 81 rows\n",
      "Merged dataset location: d:\\Financial News Sentiment Analysis\\Data\\merged_news_prices.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == \"notebooks\" else NOTEBOOK_DIR\n",
    "DATA_DIR = ROOT / \"Data\"\n",
    "MERGED_PATH = DATA_DIR / \"merged_news_prices.csv\"\n",
    "\n",
    "\n",
    "FORCE_REBUILD = True  \n",
    "MAX_TICKERS = 100   \n",
    "DOWNLOAD_MISSING = True  \n",
    "DOWNLOAD_LIMIT = 1     \n",
    "\n",
    "\n",
    "\n",
    "scripts_dir = ROOT / \"scripts\"\n",
    "sys.path.insert(0, str(scripts_dir))\n",
    "\n",
    "import build_dataset\n",
    "importlib.reload(build_dataset)  \n",
    "\n",
    "if FORCE_REBUILD or not MERGED_PATH.exists():\n",
    "    build_dataset.main(\n",
    "        target_tickers=None,\n",
    "        download_missing=DOWNLOAD_MISSING,\n",
    "        max_tickers=MAX_TICKERS,\n",
    "        download_limit=DOWNLOAD_LIMIT,\n",
    "    )\n",
    "else:\n",
    "    print(\"Using existing merged dataset; set FORCE_REBUILD=True to regenerate.\")\n",
    "\n",
    "print(f\"Merged dataset location: {MERGED_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6494a2",
   "metadata": {},
   "source": [
    "### Cleaning pipeline\n",
    "\n",
    "Steps for `Headlines`:\n",
    "- Strip URLs, $TICKER strings, and finance-specific symbols.\n",
    "- Tokenize, lemmatize, and lowercase.\n",
    "- Remove stop words while keeping key finance terms: up, down, high, low, growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46148eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading punkt_tab...\n",
      "Downloading wordnet...\n",
      "Downloading omw-1.4...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk_data_dir = os.path.join(ROOT, \".venv\", \"nltk_data\")\n",
    "os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "\n",
    "if nltk_data_dir not in nltk.data.path:\n",
    "    nltk.data.path.insert(0, nltk_data_dir)\n",
    "\n",
    "resources = [\n",
    "    \"tokenizers/punkt\",\n",
    "    \"tokenizers/punkt_tab\", \n",
    "    \"corpora/stopwords\",\n",
    "    \"corpora/wordnet\",\n",
    "    \"corpora/omw-1.4\"\n",
    "]\n",
    "\n",
    "def download_nltk_resource(resource_path):\n",
    "    resource_name = resource_path.split('/')[-1]\n",
    "    try:\n",
    "        nltk.data.find(resource_path)\n",
    "    except LookupError:\n",
    "        print(f\"Downloading {resource_name}...\")\n",
    "        nltk.download(resource_name, download_dir=nltk_data_dir, quiet=True)\n",
    "\n",
    "\n",
    "for res in resources:\n",
    "    download_nltk_resource(res)\n",
    "\n",
    "\n",
    "base_stop = set(stopwords.words(\"english\"))\n",
    "keep_terms = {\"up\", \"down\", \"high\", \"low\", \"growth\", \"above\", \"below\"}\n",
    "custom_stop = base_stop - keep_terms\n",
    "\n",
    "\n",
    "url_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "ticker_pattern = re.compile(r\"\\$[A-Za-z]{1,10}\")\n",
    "finance_symbol_pattern = re.compile(r\"[\\$€£¥%]\")  \n",
    "non_alpha_pattern = re.compile(r\"[^a-zA-Z\\s]\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_headline(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    \n",
    "    text = url_pattern.sub(\" \", text)\n",
    "    text = ticker_pattern.sub(\" \", text)\n",
    "    text = finance_symbol_pattern.sub(\" \", text)\n",
    "    text = non_alpha_pattern.sub(\" \", text)\n",
    "    \n",
    "    \n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    tokens = [lemmatizer.lemmatize(tok) for tok in tokens if tok not in custom_stop and len(tok) > 1]\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb7c798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date Ticker        Open        High         Low       Close  \\\n",
      "0 2020-06-09   AAPL   83.035004   86.402496   83.002502   85.997498   \n",
      "1 2020-06-09   AMZN  126.472000  131.321503  126.250000  130.042999   \n",
      "2 2011-05-23    DNO   38.970001   39.090000   38.700001   38.779999   \n",
      "3 2011-06-08    DNO   37.889999   37.889999   37.040001   37.389999   \n",
      "4 2011-07-01    DNO   39.889999   40.160000   39.459999   39.650002   \n",
      "\n",
      "    Adj Close     Volume                                          Headlines  \\\n",
      "0   83.889359  147712400  Why Apple's Stock Is Trading Higher Today Appl...   \n",
      "1  130.042999  103520000  'Inside Amazon's plan to test warehouse worker...   \n",
      "2   38.779999      13400      American Drivers Should Thank European Voters   \n",
      "3   37.389999      38900                                   The End of OPEC?   \n",
      "4   39.650002       9100  Is China's Slowdown Bullish for the Global Eco...   \n",
      "\n",
      "   Target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       0  \n",
      "3       1  \n",
      "4       1  \n"
     ]
    }
   ],
   "source": [
    "# Load merged dataset\n",
    "merged = pd.read_csv(MERGED_PATH, parse_dates=[\"Date\"])\n",
    "print(merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c9a301e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date Ticker                                          Headlines  \\\n",
      "0 2020-06-09   AAPL  Why Apple's Stock Is Trading Higher Today Appl...   \n",
      "1 2020-06-09   AMZN  'Inside Amazon's plan to test warehouse worker...   \n",
      "2 2011-05-23    DNO      American Drivers Should Thank European Voters   \n",
      "3 2011-06-08    DNO                                   The End of OPEC?   \n",
      "4 2011-07-01    DNO  Is China's Slowdown Bullish for the Global Eco...   \n",
      "\n",
      "                                     Headlines_clean  \n",
      "0  apple stock trading higher today apple could a...  \n",
      "1  inside amazon plan test warehouse worker covid...  \n",
      "2               american driver thank european voter  \n",
      "3                                           end opec  \n",
      "4              china slowdown bullish global economy  \n"
     ]
    }
   ],
   "source": [
    "merged[\"Headlines_clean\"] = merged[\"Headlines\"].apply(clean_headline)\n",
    "\n",
    "print(merged[[\"Date\", \"Ticker\", \"Headlines\", \"Headlines_clean\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9886a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to d:\\Financial News Sentiment Analysis\\Data\\merged_news_prices_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = DATA_DIR / \"merged_news_prices_cleaned.csv\"\n",
    "merged.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
